# Projet K-Means Clustering

## ğŸ“ Description

Ce projet implÃ©mente lâ€™algorithme **K-Means** en Python pour effectuer du **clustering** sur diffÃ©rents jeux de donnÃ©es.  
Il permet de comparer deux mÃ©thodes dâ€™initialisation des centroids : `random` et `k-means++`, et dâ€™Ã©valuer la qualitÃ© du clustering Ã  lâ€™aide des mÃ©triques **Silhouette Score** et **Davies-Bouldin Score**.

### Objectifs pÃ©dagogiques :
- Comprendre le fonctionnement de lâ€™algorithme K-Means.  
- Comparer lâ€™impact des diffÃ©rentes mÃ©thodes dâ€™initialisation.  
- Ã‰valuer les rÃ©sultats Ã  lâ€™aide de mÃ©triques de clustering.  
- Analyser des datasets rÃ©els et tirer des conclusions pertinentes.

---

## ğŸ’» Technologies utilisÃ©es

- **Python 3**
- BibliothÃ¨ques : `numpy`, `pandas`, `matplotlib`, `scikit-learn`
- Visualisation : `matplotlib.pyplot` pour les graphiques et PCA

---

## ğŸ“Š RÃ©sultats et analyses par dataset

### ğŸŸ¢ Dataset 1 : Iris
**Meilleur k : 2**  
**Initialisation : random / k-means++**  

| init      | k | silhouette | davies_bouldin |
| --------- | - | ---------- | --------------- |
| random    | 2 | 0.582      | 0.593           |
| random    | 3 | 0.459      | 0.834           |
| random    | 4 | 0.387      | 0.870           |
| random    | 5 | 0.346      | 0.948           |
| k-means++ | 2 | 0.582      | 0.593           |
| k-means++ | 3 | 0.460      | 0.834           |
| k-means++ | 4 | 0.387      | 0.870           |
| k-means++ | 5 | 0.346      | 0.948           |

*Analyse :*  
- k = 2 donne le meilleur compromis (Silhouette max, Davies-Bouldin min)  
- RÃ©sultats trÃ¨s similaires entre random et k-means++  
- Features : sepal length, sepal width, petal length, petal width

---

### ğŸŸ£ Dataset 2 : Wine
**Meilleur k : 3**  
**Initialisation : random / k-means++**

| init      | k | silhouette | davies_bouldin |
| --------- | - | ---------- | --------------- |
| random    | 2 | 0.268      | 1.448           |
| random    | 3 | 0.285      | 1.389           |
| random    | 4 | 0.252      | 1.817           |
| random    | 5 | 0.245      | 1.736           |
| k-means++ | 2 | 0.259      | 1.526           |
| k-means++ | 3 | 0.285      | 1.389           |
| k-means++ | 4 | 0.260      | 1.797           |
| k-means++ | 5 | 0.202      | 1.808           |

*Analyse :*  
- k = 3 cohÃ©rent avec les 3 classes rÃ©elles  
- random et k-means++ donnent rÃ©sultats identiques pour k=3  
- Features : alcohol, malic_acid, ash, alcalinity_of_ash, magnesium, total_phenols, flavanoids, nonflavanoid_phenols, proanthocyanins, color_intensity, hue, od280/od315, proline

---

### ğŸŸ¡ Dataset 3 : Mall Customers
**Meilleur k : 5**  
**Initialisation : k-means++**

| init      | k | silhouette | davies_bouldin |
| --------- | - | ---------- | --------------- |
| random    | 2 | 0.252      | 1.614           |
| random    | 3 | 0.260      | 1.357           |
| random    | 4 | 0.297      | 1.296           |
| random    | 5 | 0.304      | 1.160           |
| k-means++ | 2 | 0.252      | 1.614           |
| k-means++ | 3 | 0.260      | 1.357           |
| k-means++ | 4 | 0.298      | 1.281           |
| k-means++ | 5 | 0.304      | 1.167           |

*Analyse :*  
- k = 5 logique pour segmenter diffÃ©rents profils clients  
- k-means++ lÃ©gÃ¨rement plus stable  
- Features : Age, Annual Income, Spending Score

---

### ğŸŸ£ Dataset 4 : Breast Cancer
**Meilleur k : 2**  
**Initialisation : random / k-means++**

| init      | k | silhouette | davies_bouldin |
| --------- | - | ---------- | --------------- |
| random    | 2 | 0.343      | 1.321           |
| random    | 3 | 0.314      | 1.529           |
| random    | 4 | 0.271      | 1.513           |
| random    | 5 | 0.176      | 1.725           |
| k-means++ | 2 | 0.343      | 1.321           |
| k-means++ | 3 | 0.314      | 1.529           |
| k-means++ | 4 | 0.283      | 1.489           |
| k-means++ | 5 | 0.158      | 1.756           |

*Analyse :*  
- k = 2 correspond aux classes bÃ©nin / malin  
- random et k-means++ donnent rÃ©sultats proches  
- Features : 30 variables (mean radius, mean texture, mean perimeter, â€¦)

---

### âœ… Conclusion gÃ©nÃ©rale

| Dataset        | k optimal | Initialisation recommandÃ©e   |
| -------------- | --------- | ---------------------------- |
| Iris           | 2         | random / k-means++           |
| Wine           | 3         | random / k-means++           |
| Mall Customers | 5         | k-means++                    |
| Breast Cancer  | 2         | random / k-means++           |

*SynthÃ¨se :*  
- Le meilleur k varie selon les datasets  
- Silhouette Ã  maximiser, Davies-Bouldin Ã  minimiser â†’ rÃ©sultats cohÃ©rents  
- k-means++ plus stable et souvent lÃ©gÃ¨rement meilleur

